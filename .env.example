# Environment Configuration Template
# Copy this file to .env and update with your values

# LLM Configuration
# Ollama server URL - default is local instance
OLLAMA_BASE_URL=http://localhost:11434
# Model to use (recommended: qwen3:8b for better reasoning, or llama3.1:8b)
OLLAMA_MODEL=llama3.1:8b
# Temperature controls randomness (0.0 = deterministic, 1.0 = creative)
OLLAMA_TEMPERATURE=0.7

# LangSmith Configuration
# Enable LangSmith tracing for observability and debugging
LANGCHAIN_TRACING_V2=true
# LangSmith API key - get from https://smith.langchain.com/
LANGCHAIN_API_KEY=your_langsmith_api_key_here
# Project name in LangSmith dashboard
LANGCHAIN_PROJECT=procurement-agent

# Data Configuration
# Path to the dataset directory (relative or absolute)
DATA_PATH=Agents - Code Challenge/Data/
